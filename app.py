import joblib
import folium
import numpy as np
import pandas as pd
import seaborn as sns
from PIL import Image
import streamlit as st
import matplotlib.pyplot as plt
from geopy.distance import geodesic
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
pd.set_option('display.max_columns', None)

# Verileri yÃ¼kle
df_ = pd.read_csv(
    "traffic_density__202407.csv")
df = df_.copy()
df["DATE_TIME"] = pd.to_datetime(df["DATE_TIME"], format="%Y-%m-%d %H:%M:%S", errors="coerce")

dfpark_ = pd.read_excel(
    "istanbul-park-ve-yeil-alan-koordinatlar.xlsx")
dfpark = dfpark_.copy()

# Sidebar ile sayfa seÃ§imi
st.sidebar.title("NETAÅ")
image = Image.open('logo2.png')  # 'logo.png' dosyasÄ±nÄ±n doÄŸru yerde olduÄŸundan emin olun
with st.sidebar:  # 'with' bloÄŸu dÃ¼zeltildi
    st.image(image, caption='NETAÅ', use_column_width=True)
    st.write("Yeteneklerimizi Sizin iÃ§in KullanÄ±yoruz!")
page = st.sidebar.radio("Gitmek istediÄŸiniz sayfayÄ± seÃ§in:", ["Data Table", "Acil Durum AlanlarÄ±", "Ä°letiÅŸim"])

# Sayfa 1: Data Tablosu
if page == "Data Table":
    st.title("AkÄ±llÄ± Acil Durum YÃ¶netimi")
    st.write("Trafik YoÄŸunluÄŸu Verisi")
    st.dataframe(df)

    st.write("Park ve YeÅŸil Alan Verisi")
    st.dataframe(dfpark)


    def time_series_feature_engineering(df):
        # DATE_TIME kolonunu datetime formatÄ±na Ã§evir
        df['DATE_TIME'] = pd.to_datetime(df['DATE_TIME'])

        # 1. Zaman BileÅŸenleri
        df['year'] = df['DATE_TIME'].dt.year
        df['month'] = df['DATE_TIME'].dt.month
        df['day'] = df['DATE_TIME'].dt.day
        df['hour'] = df['DATE_TIME'].dt.hour
        df['day_of_week'] = df['DATE_TIME'].dt.dayofweek  # 0 = Pazartesi, 6 = Pazar
        df['weekend'] = df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)  # Hafta sonu bilgisi

        # 2. DÃ¶nemsellik Ã–zellikleri
        df['day_of_year'] = df['DATE_TIME'].dt.dayofyear
        df['week_of_year'] = df['DATE_TIME'].dt.isocalendar().week

        # 3. Hareketli Ä°statistikler
        df['rolling_avg_speed'] = df['AVERAGE_SPEED'].rolling(window=3,
                                                              min_periods=1).mean()  # 3 zaman dilimi hareketli ortalama
        df['rolling_min_speed'] = df['MINIMUM_SPEED'].rolling(window=3, min_periods=1).min()
        df['rolling_max_speed'] = df['MAXIMUM_SPEED'].rolling(window=3, min_periods=1).max()
        df['rolling_vehicle_count'] = df['NUMBER_OF_VEHICLES'].rolling(window=3,
                                                                       min_periods=1).sum()  # AraÃ§ sayÄ±sÄ±nÄ±n toplamÄ±

        # 4. Lag (Gecikme) Ã–zellikleri
        df['lag_1_avg_speed'] = df['AVERAGE_SPEED'].shift(1)  # Bir Ã¶nceki zaman dilimindeki ortalama hÄ±z
        df['lag_1_vehicle_count'] = df['NUMBER_OF_VEHICLES'].shift(1)  # Bir Ã¶nceki araÃ§ sayÄ±sÄ±

        # 5. HÄ±z Trendleri
        df['speed_diff'] = df['AVERAGE_SPEED'] - df['lag_1_avg_speed']  # HÄ±z deÄŸiÅŸimi
        df['vehicle_diff'] = df['NUMBER_OF_VEHICLES'] - df['lag_1_vehicle_count']  # AraÃ§ sayÄ±sÄ± deÄŸiÅŸimi

        # BoÅŸ deÄŸerler varsa doldur (Ã¶rneÄŸin gecikme kolonlarÄ±ndan kaynaklÄ±)
        df.fillna(0, inplace=True)

        return df


    df = time_series_feature_engineering(df)


    def preprocess_data(df):
        """
        Preprocessing adÄ±mlarÄ±nÄ± iÃ§eren fonksiyon.
        Args:
        df: DataFrame
        Returns:
        df_preprocessed: Preprocessed DataFrame
        """
        # 1. String olan kolonlarÄ± sayÄ±sallaÅŸtÄ±r
        # GEOHASH gibi kolonlarÄ± sayÄ±sallaÅŸtÄ±rmak iÃ§in Label Encoding uyguladÄ±k
        label_enc = LabelEncoder()
        df['GEOHASH'] = label_enc.fit_transform(df['GEOHASH'].astype(str))

        # 3. Ã–zellikleri normalize veya standardize et
        # Normalizasyon yerine StandardScaler tercih ettik
        feature_columns = df.drop(["LATITUDE", "LONGITUDE", "DATE_TIME", "NUMBER_OF_VEHICLES"], axis=1).columns
        scaler = StandardScaler()

        df[feature_columns] = scaler.fit_transform(df[feature_columns])

        return df


    # Data preprocessing adÄ±mÄ±nÄ± Ã§aÄŸÄ±r
    df = preprocess_data(df)
    df = df.head(1999)
    df_ = df_.head(1999)
    st.session_state.df = df
    st.session_state.df_ = df_

# Sayfa 2: Acil Durum AlanlarÄ±
elif page == "Acil Durum AlanlarÄ±":
    st.title("Acil Durum Toplanma AlanlarÄ±")

    df = st.session_state.df
    df_ = st.session_state.df_


    def train_and_evaluate_model(df, target_column):
        """
        EÄŸitim ve test verilerini bÃ¶lÃ¼p RandomForestRegressor modeli eÄŸitir ve deÄŸerlendirir.
        Args:
        df: DataFrame, verisetini temsil eder.
        feature_columns: List, modelde kullanÄ±lacak Ã¶zellik kolonlarÄ±.
        target_column: String, hedef kolon adÄ± (NUMBER_OF_VEHICLES).
        Returns:
        model: EÄŸitilmiÅŸ RandomForestRegressor nesnesi.
        y_test: Test verisi hedef deÄŸerleri.
        y_pred: Test verisi tahminleri.
        """
        feature_columns = df.drop(["LATITUDE", "LONGITUDE", "DATE_TIME", "NUMBER_OF_VEHICLES"], axis=1).columns
        # Ã–zellikleri ve hedefi oluÅŸtur
        X = df[feature_columns]
        y = df[target_column]

        # EÄŸitim ve test verilerini bÃ¶l
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # RandomForestRegressor modelini oluÅŸtur ve eÄŸit
        model = RandomForestRegressor(n_estimators=200, random_state=42)
        model.fit(X_train, y_train)

        # Test verisi Ã¼zerinde tahminleme yap
        y_pred = model.predict(X_test)

        # Model performansÄ±nÄ± deÄŸerlendir
        mae = mean_absolute_error(y_test, y_pred)
        mse = mean_squared_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)

        print("\nModel PerformansÄ±:")
        print("Mean Absolute Error:", mae)
        print("Mean Squared Error:", mse)
        print("RÂ² Score:", r2)
        st.write("\n**Model PerformansÄ±:**")
        st.write(f"âœ… Mean Absolute Error (MAE): {mae}")
        st.write(f"âœ… Mean Squared Error (MSE): {mse}")
        st.write(f"âœ… RÂ² Score: {r2}")

        # Grafikle sonuÃ§larÄ± gÃ¶rselleÅŸtir
        plt.figure(figsize=(10, 5))
        plt.plot(y_test.values, label='True Vehicle Counts')
        plt.plot(y_pred, label='Predicted Vehicle Counts', alpha=0.7)
        plt.title("True vs Predicted Vehicle Counts")
        plt.legend(loc="upper left")
        plt.show()

        return model, X_test, y_test, y_pred


    # Modeli eÄŸit ve deÄŸerlendir

    target_column = 'NUMBER_OF_VEHICLES'

    # Fonksiyonu Ã§aÄŸÄ±r
    model, X_test, y_test, y_pred = train_and_evaluate_model(df, target_column)


    def predict_and_find_dense_areas(model, df, df_):
        # Model tahminlemesini yap ve yeni sÃ¼tunlara ekle
        df_['predicted_vehicle_count'] = model.predict(
            df.drop(["LATITUDE", "LONGITUDE", "DATE_TIME", "NUMBER_OF_VEHICLES"], axis=1))

        # Tahmin edilen verileri tablo ÅŸeklinde gÃ¶ster
        prediction_table = df_[['DATE_TIME', 'GEOHASH', 'LATITUDE', 'LONGITUDE', 'predicted_vehicle_count']]
        print("Tahminlenen AraÃ§ YoÄŸunluklarÄ±:")
        print(prediction_table.head())

        # YoÄŸun bÃ¶lgeleri tespit et
        # Burada araÃ§ sayÄ±sÄ± ortalamanÄ±n Ã¼zerinde ve hÄ±z ortalamasÄ±nÄ±n altÄ±nda olan bÃ¶lgeleri buluyoruz
        # Model tahminlemesi yap

        # Her geohash iÃ§in ortalama araÃ§ sayÄ±sÄ±nÄ± hesapla
        geohash_avg_vehicle_count = df_.groupby('GEOHASH')['NUMBER_OF_VEHICLES'].mean().to_dict()

        # Tahmin edilen verileri kontrol et ve yoÄŸun bÃ¶lgeleri ayÄ±r
        dense_areas = []
        for idx, row in df_.iterrows():
            geohash = row['GEOHASH']
            predicted_count = row['predicted_vehicle_count']
            avg_count = geohash_avg_vehicle_count.get(geohash, 0)

            # EÄŸer tahmin edilen araÃ§ sayÄ±sÄ±, ortalama araÃ§ sayÄ±sÄ±ndan bÃ¼yÃ¼kse yoÄŸun olabilir
            if predicted_count > avg_count:
                dense_areas.append(row)

        # YoÄŸun bÃ¶lgeleri yeni DataFrame'e al
        dense_df = pd.DataFrame(dense_areas)

        # SonuÃ§larÄ± kontrolle
        print("YoÄŸun bÃ¶lgelerin DataFrame'i:")
        print(dense_df[['DATE_TIME', 'GEOHASH', 'LATITUDE', 'LONGITUDE', 'predicted_vehicle_count']])

        return prediction_table, dense_df, df_


    prediction_table, dense_df, df_ = predict_and_find_dense_areas(model, df, df_)
    st.dataframe(prediction_table)

    # NaN deÄŸerleri kaldÄ±rma
    dense_df.dropna(subset=['LATITUDE', 'LONGITUDE'], inplace=True)
    dfpark.dropna(subset=['LATITUDE', 'LONGITUDE'], inplace=True)
    dfpark.columns = dfpark.columns.str.strip()


    st.title("Acil durum esnasÄ±nda en yakÄ±n park ve yeÅŸil alan")
    matched_df = pd.read_csv("matched_data.csv")
    st.dataframe(matched_df)

    # Streamlit BaÅŸlÄ±ÄŸÄ±
    st.title('Park ve Dense EÅŸleÅŸmeleri HaritasÄ±')
    # Rastgele bir eÅŸleÅŸme seÃ§
    if not matched_df.empty:
        sampled_matched = matched_df.sample(n=1)

        # SÃ¼tunlarÄ± sayÄ±ya Ã§evir ve NaN deÄŸerleri temizle
        columns_to_convert = ['LATITUDE_DENSE', 'LONGITUDE_DENSE', 'LATITUDE_PARK', 'LONGITUDE_PARK']
        for col in columns_to_convert:
            sampled_matched[col] = pd.to_numeric(sampled_matched[col], errors='coerce')

        sampled_matched = sampled_matched.dropna(subset=columns_to_convert)

        # EÄŸer geÃ§erli satÄ±r kalmadÄ±ysa Ã§Ä±kÄ±ÅŸ yap
        if sampled_matched.empty:
            st.write("GeÃ§erli veri bulunamadÄ±.")
        else:
            row = sampled_matched.iloc[0]
            dense_lat, dense_lon = row['LATITUDE_DENSE'], row['LONGITUDE_DENSE']
            park_lat, park_lon = row['LATITUDE_PARK'], row['LONGITUDE_PARK']

            # Folium haritasÄ± oluÅŸtur
            m = folium.Map(location=[(dense_lat + park_lat) / 2, (dense_lon + park_lon) / 2], zoom_start=15)

            # Yol noktalarÄ±nÄ± ekle
            folium.Marker([dense_lat, dense_lon], popup='Dense (Road)', icon=folium.Icon(color='red')).add_to(m)
            folium.Marker([park_lat, park_lon], popup='Park', icon=folium.Icon(color='green')).add_to(m)

            # Park ve Dense noktalarÄ±nÄ± baÄŸlayan Ã§izgi ekle
            folium.PolyLine([(dense_lat, dense_lon), (park_lat, park_lon)], color='blue', weight=2).add_to(m)

            # Mesafe hesaplama
            distance = geodesic((dense_lat, dense_lon), (park_lat, park_lon)).km

            st.write(f"Park ve Dense noktasÄ± arasÄ±ndaki mesafe: {distance:.2f} km")

            # HaritayÄ± kaydet ve gÃ¶ster
            m.save('map.html')
            st.components.v1.html(open('map.html', 'r').read(), height=600)

    else:
        st.write("Veri bulunamadÄ±.")
# Sayfa 3: Ä°letiÅŸim
elif page == "Ä°letiÅŸim":
    st.title("Ä°letiÅŸim")
    st.write("""
                    **Daha Fazla Soru ve Ä°letiÅŸim Ä°Ã§in**  
                    Bu projeyle ilgili herhangi bir sorunuz veya geri bildiriminiz olursa benimle iletiÅŸime geÃ§mekten Ã§ekinmeyin! AÅŸaÄŸÄ±daki platformlar Ã¼zerinden ulaÅŸabilirsiniz:
                    """)

    st.write("ğŸ“§ **E-posta**: furkansukan10@gmail.com")
    st.write("ğŸªª **LinkedIn**: https://www.linkedin.com/in/furkansukan/")
    st.write("ğŸ”— **Kaggle**: https://www.kaggle.com/furkansukan")
    st.write("ğŸ™ **GitHub**: https://github.com/furkansukan")  # Buraya baÄŸlantÄ± ekleyebilirsiniz
    # Buraya baÄŸlantÄ± ekleyebilirsiniz

    st.write("""
                    GÃ¶rÃ¼ÅŸ ve Ã¶nerilerinizi duymaktan mutluluk duyarÄ±m!
                    """)
